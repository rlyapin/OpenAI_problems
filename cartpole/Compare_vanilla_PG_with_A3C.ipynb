{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from rl_agent import RL_Agent\n",
    "from rl_learner import PG_Learner\n",
    "from rl_learner import A3C_Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cartpole_Agent(RL_Agent):\n",
    "    # Overwriting supposedly abstract RL_Agent class\n",
    "    # All what is left is to actually provide the specific model to choose action\n",
    "    # It is still implied that\n",
    "    # 1) __init__ method defines all its variables in model_name scope\n",
    "    # 2) the class has self.session, self.prob_layer and self.log_prob_layer methods\n",
    "    # The remaining functionality needed in PG and TRPO learners is still defined in abstract base\n",
    "    def __init__(self, model_name):\n",
    "        RL_Agent.__init__(self, model_name)\n",
    "        with tf.variable_scope(model_name):\n",
    "            self.session = tf.Session()\n",
    "            self.n_actions = 2\n",
    "            \n",
    "            self.input_layer = tf.placeholder(shape=[None, 4], dtype=tf.float32)\n",
    "            self.dense1_layer = tf.layers.dense(self.input_layer, \n",
    "                                                units=4, use_bias=True, \n",
    "                                                activation=tf.nn.relu, name=\"dense1_weights\"\n",
    "                                               )\n",
    "            \n",
    "            self.dense2_layer = tf.layers.dense(self.dense1_layer, \n",
    "                                                units=2, use_bias=True, \n",
    "                                                activation=tf.nn.relu, name=\"dense2_weights\"\n",
    "                                               ) \n",
    "            \n",
    "            self.prob_layer = tf.nn.softmax(self.dense2_layer)\n",
    "            self.log_prob_layer = tf.log(self.prob_layer)\n",
    "            \n",
    "            self.state_value = tf.layers.dense(self.dense1_layer, units=1, use_bias=True, activation=None) \n",
    "                        \n",
    "            self.session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "pg = PG_Learner(rl_agent=Cartpole_Agent(\"2018_02_02_cartpole_pg\"), \n",
    "                game_env=env,\n",
    "                discount=0.99, \n",
    "                batch_size=100, \n",
    "                frame_cap=None,\n",
    "                lr=0.01)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(100):\n",
    "    pg.step()\n",
    "print \"Used time: {} seconds\".format(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pg.reward_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considering Actor-Critic learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "rl_agent = Cartpole_Agent(\"2018_02_02_cartpole_pg\")\n",
    "\n",
    "a3c = A3C_Learner(actor_agent=rl_agent, \n",
    "                  critic_agent=rl_agent,\n",
    "                  game_env=env,\n",
    "                  discount=0.99, \n",
    "                  batch_size=100, \n",
    "                  frame_cap=None,\n",
    "                  actor_lr=0.01,\n",
    "                  critic_lr=0.0001)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(100):\n",
    "    states, actions, rewards = a3c.play_single_game()\n",
    "    baselines = a3c.critic_agent.evaluate_states(states)\n",
    "#     print rewards, baselines\n",
    "    print sum(actions) / float(len(actions)), sum(baselines) / float(len(baselines))\n",
    "    a3c.step()\n",
    "print \"Used time: {} seconds\".format(time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
