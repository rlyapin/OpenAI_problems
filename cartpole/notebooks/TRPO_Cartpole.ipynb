{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining TRPO setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def var_size(v):\n",
    "    return int(np.prod([int(d) for d in v.shape]))\n",
    "\n",
    "def get_padded_gradients(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return [g if g is not None else tf.zeros(v.shape)\n",
    "            for g, v in zip(grads, var_list)]\n",
    "\n",
    "def get_flattened_gradients(loss, var_list):\n",
    "    padded_gradients = get_padded_gradients(loss, var_list)\n",
    "    return tf.concat([tf.reshape(x, [-1]) for x in padded_gradients], 0)\n",
    "\n",
    "def unflatten_gradient(grad, var_list):\n",
    "    shapes = [v.shape for v in var_list]\n",
    "    sizes = [var_size(v) for v in var_list]\n",
    "    grads = []\n",
    "\n",
    "    pointer = 0\n",
    "    for (shape, size, v) in zip(shapes, sizes, var_list):\n",
    "        grads.append(tf.reshape(grad[pointer:pointer + size], shape))\n",
    "        pointer += size\n",
    "    return grads\n",
    "\n",
    "def sum_discounted_rewards(rewards, discount):\n",
    "    discounted_rewards = list(rewards)\n",
    "    pointer = len(rewards) - 1\n",
    "    acc_discounted_sum = rewards[-1]\n",
    "    while pointer > 0:\n",
    "        acc_discounted_sum *= 0.95\n",
    "        pointer -= 1\n",
    "        discounted_rewards[pointer] += acc_discounted_sum\n",
    "        acc_discounted_sum += rewards[pointer]\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cg_solver(Ax_fun, b, k_iter=10, eps=10**(-6)):\n",
    "    # Solver for Ax = b that uses conjugate gradient method\n",
    "    # Ax_fun is supposed to be a function that takes x and returns Ax\n",
    "    # Impementation of the solver is taken from Wikipedia article\n",
    "    # Solver itself is supposed to be outside tf realm - all vector inside are np arrays\n",
    "    # Note that it is assumed k_iter < dim(b)\n",
    "    x = np.zeros(b.shape, dtype=np.float)\n",
    "    r = b - Ax_fun(x)\n",
    "    p = np.array(r)\n",
    "    for k in range(k_iter):\n",
    "        \n",
    "        r_norm = float(np.sum(r * r))\n",
    "        A_p = Ax_fun(p)\n",
    "        alpha = r_norm / np.sum(p * A_p)\n",
    "        x += alpha * p\n",
    "        r -= alpha * A_p\n",
    "        next_r_norm = np.sum(r * r)\n",
    "        \n",
    "        if next_r_norm < eps:\n",
    "            break\n",
    "            \n",
    "        beta = next_r_norm / r_norm\n",
    "        p *= beta\n",
    "        p += r\n",
    "        \n",
    "    return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RL_Agent:\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        with tf.variable_scope(model_name):\n",
    "            self.model_name = model_name\n",
    "            self.session = tf.Session()\n",
    "            \n",
    "            self.input_layer = tf.placeholder(shape=[None, 4], dtype=tf.float32)\n",
    "            self.dense1_layer = tf.layers.dense(self.input_layer, \n",
    "                                                units=4, use_bias=True, \n",
    "                                                activation=tf.nn.relu, name=\"dense1_weights\"\n",
    "                                               )\n",
    "            \n",
    "            self.dense2_layer = tf.layers.dense(self.dense1_layer, \n",
    "                                                units=2, use_bias=True, \n",
    "                                                activation=tf.nn.relu, name=\"dense2_weights\"\n",
    "                                               ) \n",
    "            \n",
    "            self.prob_layer = tf.maximum(tf.minimum(tf.nn.softmax(self.dense2_layer), 0.9999), 0.0001)\n",
    "            self.log_prob_layer = tf.log(self.prob_layer)\n",
    "                        \n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def model_variables(self):\n",
    "        return [x for x in tf.trainable_variables() if self.model_name in x.name]\n",
    "    \n",
    "    def model_size(self):\n",
    "        var_sizes = [tf.size(x) for x in self.model_variables()]\n",
    "        return self.session.run(tf.reduce_sum(var_sizes))\n",
    "    \n",
    "    def variable_size(self):\n",
    "        var_sizes = [tf.size(x) for x in self.model_variables()]\n",
    "        return self.session.run(var_sizes)\n",
    "            \n",
    "    def predict(self, states):\n",
    "        return self.session.run(self.prob_layer, feed_dict={self.input_layer: states})\n",
    "    \n",
    "    def pg_grad(self, states, actions, rewards):\n",
    "        # Calculating base policy gradient\n",
    "        # Return a sum of log_prob gradients weighted by discounted sum of future rewards\n",
    "        action_mask = tf.one_hot(actions, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "        picked_log_prob_actions = tf.reduce_sum(action_mask * self.log_prob_layer, axis=1)\n",
    "        weighted_log_prob_actions = picked_log_prob_actions * rewards\n",
    "        grad_log_prob_actions = get_flattened_gradients(weighted_log_prob_actions, self.model_variables())\n",
    "        return self.session.run(grad_log_prob_actions, feed_dict={self.input_layer: states})\n",
    "\n",
    "    def trpo_grad(self, states, actions, rewards):\n",
    "        # Calculating the target gradient as defined in section 5 of the trpo paper\n",
    "        # Takes a gradient of prob action ratio weighted by Q-values\n",
    "        # Essentially it should be a gradient for compare_prob_ratios function\n",
    "        action_mask = tf.one_hot(actions, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "        fixed_prob_actions = tf.stop_gradient(self.prob_layer)\n",
    "        prob_ratio = self.prob_layer / fixed_prob_actions\n",
    "        masked_prob_ratio = tf.reduce_sum(action_mask * prob_ratio, axis=1)\n",
    "        weighted_prob_ratio = masked_prob_ratio * rewards\n",
    "        trpo_gradient = get_flattened_gradients(weighted_prob_ratio, self.model_variables())\n",
    "        return self.session.run(trpo_gradient, feed_dict={self.input_layer: states})\n",
    "    \n",
    "    def fisher_vector_product(self, states, vector):\n",
    "        # This function is supposed to return the product of estimated fisher information matrix and a specified vector\n",
    "        # As I hope to reliably estimate this matrix, I take all states accumulated in a batch of games\n",
    "        expected_log_prob = tf.reduce_sum(tf.stop_gradient(self.prob_layer) * self.log_prob_layer, axis=1)\n",
    "        log_prob_grad = get_flattened_gradients(expected_log_prob, self.model_variables())\n",
    "        grad_vector_product = tf.reduce_sum(log_prob_grad * vector)\n",
    "        fisher_vector_product = -get_flattened_gradients(grad_vector_product, self.model_variables()) / states.shape[0]\n",
    "        return self.session.run(fisher_vector_product, feed_dict={self.input_layer: states})        \n",
    "    \n",
    "    def compare_prob_ratios(self, states, actions, rewards, d_var):\n",
    "        # Estimates the function to optimize as defined is section 5 of original paper\n",
    "        # I take the ratio of probabilities from original weights and probs from proposed weights (var + d_var)\n",
    "        # These ratios are weighted by Q-values: thus, if output > 1 new weights make better actions more probable\n",
    "        # Also, wighout scaling by original probs, its gradient is given by grad_log_prob_actions\n",
    "        action_mask = tf.one_hot(actions, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "        \n",
    "        original_prob_actions = tf.reduce_sum(action_mask * self.prob_layer, axis=1)\n",
    "        np_original_prob_actions = self.session.run(original_prob_actions, feed_dict={self.input_layer: states})\n",
    "\n",
    "        for (grad, var) in zip(d_var, self.model_variables()):\n",
    "            self.session.run(tf.assign_add(var, grad))\n",
    "            \n",
    "        new_prob_actions = tf.reduce_sum(action_mask * self.prob_layer, axis=1)   \n",
    "        np_new_prob_actions = self.session.run(new_prob_actions, feed_dict={self.input_layer: states})\n",
    "\n",
    "        for (grad, var) in zip(d_var, self.model_variables()):\n",
    "            self.session.run(tf.assign_sub(var, grad))\n",
    "            \n",
    "        return sum((np_new_prob_actions / np_original_prob_actions) * np.array(rewards))\n",
    "\n",
    "    def estimate_kl_divergence(self, states, d_var):\n",
    "        # This function calculates an actual kl_divergence between action prob distributions\n",
    "        # with current weights and weight + d_var\n",
    "        original_probs = self.session.run(self.prob_layer, feed_dict={self.input_layer: states})\n",
    "        original_log_probs = self.session.run(self.log_prob_layer, feed_dict={self.input_layer: states})\n",
    "\n",
    "        for (grad, var) in zip(d_var, self.model_variables()):\n",
    "            self.session.run(tf.assign_add(var, grad))\n",
    "            \n",
    "        new_log_probs = self.session.run(self.log_prob_layer, feed_dict={self.input_layer: states})\n",
    "\n",
    "        for (grad, var) in zip(d_var, self.model_variables()):\n",
    "            self.session.run(tf.assign_sub(var, grad))\n",
    "            \n",
    "        kl = original_probs * (original_log_probs - new_log_probs)\n",
    "        return np.sum(kl) / kl.shape[0]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RL_Learner:\n",
    "    \n",
    "    def __init__(self, rl_agent, game_env, discount, batch_size):\n",
    "        self.session = rl_agent.session\n",
    "        self.agent = rl_agent\n",
    "        self.env = game_env\n",
    "        \n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.played_games = 0\n",
    "        \n",
    "    def play_single_game(self):\n",
    "        states = None\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        observation = self.env.reset().reshape((1, 4))\n",
    "        done = False\n",
    "        \n",
    "        while done == False:\n",
    "            if states is None:\n",
    "                states = observation\n",
    "            else:\n",
    "                states = np.concatenate((states, observation), axis=0)\n",
    "            prob_actions = self.agent.predict(observation)[0]\n",
    "            action = np.random.choice(np.arange(len(prob_actions)), p=prob_actions)\n",
    "            actions.append(action)\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            observation = observation.reshape((1, 4))\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        self.reward_history.append(sum(rewards))\n",
    "        self.played_games += 1\n",
    "            \n",
    "        return states, actions, rewards\n",
    "    \n",
    "    def play_batch(self):        \n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_rewards = []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            states, actions, rewards = self.play_single_game()\n",
    "            \n",
    "            all_states.append(states)\n",
    "            all_actions.append(actions)\n",
    "            all_rewards.append(sum_discounted_rewards(rewards, self.discount))\n",
    "            \n",
    "        print \"Average reward for batch #\", self.played_games / self.batch_size, \\\n",
    "              \": \", sum(self.reward_history[-self.batch_size:]) / self.batch_size\n",
    "        \n",
    "        concat_states = reduce(lambda x, y: np.concatenate((x, y), axis=0), all_states)\n",
    "        concat_actions = reduce(lambda x, y: x + y, all_actions)\n",
    "        concat_rewards = reduce(lambda x, y: x + y, all_rewards)     \n",
    "        \n",
    "        return concat_states, concat_actions, concat_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PG_Learner(RL_Learner):\n",
    "    \n",
    "    def __init__(self, rl_agent, game_env, discount, batch_size, lr=0.1):\n",
    "        RL_Learner.__init__(self, rl_agent, game_env, discount, batch_size)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        # One policy gradient step based on one batch of games\n",
    "        \n",
    "        concat_states, concat_actions, concat_rewards = self.play_batch()\n",
    "        grad_reward = self.agent.pg_grad(concat_states,\n",
    "                                         tf.constant(concat_actions),\n",
    "                                         tf.constant(concat_rewards))  / self.batch_size\n",
    "        \n",
    "        grads = unflatten_gradient(tf.constant(self.lr * grad_reward, dtype=tf.float32), self.agent.model_variables())\n",
    "        for (grad, var) in zip(grads, self.agent.model_variables()):\n",
    "            self.session.run(tf.assign_add(var, grad))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "pg = PG_Learner(rl_agent=RL_Agent(\"cartpole\"), \n",
    "                game_env=env,\n",
    "                discount=0.95, \n",
    "                batch_size=100, \n",
    "                lr=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    pg.step()\n",
    "    \n",
    "plt.plot(range(len(pg.reward_history)), pg.reward_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TRPO_Learner(RL_Learner):\n",
    "    \n",
    "    def __init__(self, rl_agent, game_env, discount, batch_size, trpo_delta=0.01, line_search_option=\"max\"):\n",
    "        RL_Learner.__init__(self, rl_agent, game_env, discount, batch_size)\n",
    "        self.trpo_delta = trpo_delta\n",
    "        # Line search options could be: \n",
    "        # * \"none\": trpo step is taken without checking actual KL-divergence\n",
    "        # * \"max\": trpo step is maximal in picked direction that satisfies KL-constraint\n",
    "        # * \"best\": trpo step is the one that satisfies the constraint and gives the best prob ratio along the line\n",
    "        self.line_search_option = line_search_option\n",
    "        \n",
    "    def scale_down_grads(self, obs_states, grads):\n",
    "        # This function adjusts trpo direction until it starts to satisfy KL-constraint\n",
    "        while self.agent.estimate_kl_divergence(obs_states, grads) > self.trpo_delta:\n",
    "            grads = [0.5 * grad for grad in grads]\n",
    "        return grads\n",
    "    \n",
    "    def line_search(self, obs_states, obs_actions, obs_rewards, grads):\n",
    "        # This function shrinks grads in various ways and picks the scaling that:\n",
    "        # * satisfies KL-constraint\n",
    "        # * gives the best weighted action prob ratio\n",
    "        deltas = [2 ** (-i) for i in range(10)] + [0]\n",
    "\n",
    "        def feasibility_check(delta):\n",
    "            trunc_grads = [delta * grad for grad in grads]\n",
    "            return self.agent.estimate_kl_divergence(obs_states, trunc_grads) <= self.trpo_delta\n",
    "        deltas = filter(feasibility_check, deltas)\n",
    "            \n",
    "        def prob_ratio(delta):\n",
    "            trunc_grads = [delta * grad for grad in grads]\n",
    "            return self.agent.compare_prob_ratios(obs_states, obs_actions, obs_rewards, trunc_grads)\n",
    "        \n",
    "        best_delta = max(deltas, key=prob_ratio)\n",
    "        return [best_delta * grad for grad in grads]\n",
    "            \n",
    "    def step(self):\n",
    "        \n",
    "        concat_states, concat_actions, concat_rewards = self.play_batch()\n",
    "        \n",
    "        grad_reward = self.agent.trpo_grad(concat_states,\n",
    "                                           tf.constant(concat_actions),\n",
    "                                           tf.constant(concat_rewards))  / self.batch_size\n",
    "        \n",
    "        Ax_fun = lambda x: self.agent.fisher_vector_product(concat_states, tf.constant(x, dtype=tf.float32))\n",
    "        \n",
    "        trpo_dir = cg_solver(Ax_fun, grad_reward)\n",
    "        scaling = np.sqrt(2 * self.trpo_delta / np.sum(trpo_dir * Ax_fun(trpo_dir)))\n",
    "        \n",
    "        grads = unflatten_gradient(tf.constant(scaling * trpo_dir, dtype=tf.float32), self.agent.model_variables())\n",
    "               \n",
    "        if self.line_search_option == \"max\":\n",
    "            grads = self.scale_down_grads(concat_states, grads)\n",
    "        elif self.line_search_option == \"best\":\n",
    "            grads = self.line_search(concat_states, concat_actions, concat_rewards, grads)  \n",
    "                \n",
    "        for (grad, var) in zip(grads, self.agent.model_variables()):\n",
    "            self.session.run(tf.assign_add(var, grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "trpo = TRPO_Learner(rl_agent=RL_Agent(\"cartpole\"), \n",
    "                    game_env=env,\n",
    "                    discount=0.95, \n",
    "                    batch_size=100, \n",
    "                    trpo_delta=0.01,\n",
    "                    line_search_option=\"max\")\n",
    "\n",
    "for i in range(100):\n",
    "    trpo.step()\n",
    "    \n",
    "plt.plot(range(len(trpo.reward_history)), trpo.reward_history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
