{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper: https://arxiv.org/pdf/1502.05477.pdf\n",
    "\n",
    "Some help: https://github.com/wojzaremba/trpo/blob/master/main.py\n",
    "\n",
    "Main help: https://github.com/tensorflow/models/blob/master/pcl_rl/trust_region.py\n",
    "\n",
    "Sketch of proof for KL expression via Fisher Information matrix (another proof simply uses Taylor expansion): https://stats.stackexchange.com/questions/51185/connection-between-fisher-metric-and-the-relative-entropy\n",
    "\n",
    "Short reference: https://roosephu.github.io/2016/11/19/TRPO/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_ple\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ob = env.reset()\n",
    "ob = env.step(0)[0]\n",
    "print ob.shape\n",
    "plt.imshow(ob)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating products with Fisher information matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_layer = tf.placeholder(shape=[1, 512, 288, 3], dtype=tf.int32)\n",
    "true_pred = tf.constant(1, shape=[1, 1], dtype=tf.float32)\n",
    "\n",
    "conv1_layer = tf.layers.conv2d(tf.cast(input_layer, tf.float32), filters=8, kernel_size=[5, 5], \n",
    "                               padding=\"same\", use_bias=False, activation=tf.nn.relu, name=\"conv_filters\")\n",
    "pool1_layer = tf.layers.max_pooling2d(conv1_layer, pool_size=[16, 8], strides=[16, 8])\n",
    "\n",
    "flatten_layer = tf.contrib.layers.flatten(pool1_layer)\n",
    "pred_layer = tf.sigmoid(tf.layers.dense(flatten_layer, 1, use_bias=False, name=\"dense_weights\"))\n",
    "\n",
    "loss = tf.losses.mean_squared_error(pred_layer, true_pred)\n",
    "\n",
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padded_gradients(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return [g if g is not None else tf.zeros(v.shape)\n",
    "            for g, v in zip(grads, var_list)]\n",
    "\n",
    "def get_flattened_gradients(loss, var_list):\n",
    "    padded_gradients = get_padded_gradients(loss, var_list)\n",
    "    return tf.concat([tf.reshape(x, [-1]) for x in padded_gradients], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_v = tf.constant([1] * 9816, dtype=tf.float32)\n",
    "\n",
    "flat_grad = get_flattened_gradients(loss, tf.trainable_variables())\n",
    "flat_vars = tf.concat([tf.reshape(x, [-1]) for x in tf.trainable_variables()], 0)\n",
    "\n",
    "grad_vector_product = tf.reduce_sum(flat_grad * target_v)\n",
    "fisher_vector_product = get_flattened_gradients(grad_vector_product, tf.trainable_variables())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed_dict = {input_layer: np.expand_dims(ob, axis=0)}\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict=feed_dict)\n",
    "    print sess.run(pred_layer)\n",
    "    fisher_test, var_test, grad_test = sess.run([fisher_vector_product, flat_vars, flat_grad], feed_dict=feed_dict)\n",
    "    \n",
    "    print fisher_test\n",
    "    print var_test\n",
    "    print grad_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining RL agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_padded_gradients(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return [g if g is not None else tf.zeros(v.shape)\n",
    "            for g, v in zip(grads, var_list)]\n",
    "\n",
    "def get_flattened_gradients(loss, var_list):\n",
    "    padded_gradients = get_padded_gradients(loss, var_list)\n",
    "    return tf.concat([tf.reshape(x, [-1]) for x in padded_gradients], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix as in TRPO paper can be calculated via E_state [ - sum[ p(a | theta_old, state) * grad^2_theta log(p(a | theta_old, state)) ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RL_Agent:\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        with tf.variable_scope(model_name):\n",
    "            self.model_name = model_name\n",
    "            self.session = tf.Session()\n",
    "            \n",
    "            self.input_layer = tf.placeholder(shape=[None, 512, 288, 3], dtype=tf.float32)\n",
    "            self.conv1_layer = tf.layers.conv2d(self.input_layer, \n",
    "                                                filters=8, kernel_size=[5, 5], \n",
    "                                                padding=\"same\", use_bias=False, \n",
    "                                                activation=tf.nn.relu, name=\"conv_weights\"\n",
    "                                               )\n",
    "            \n",
    "            self.pool1_layer = tf.layers.max_pooling2d(self.conv1_layer, pool_size=[16, 8], strides=[16, 8])\n",
    "            self.flatten_layer = tf.contrib.layers.flatten(self.pool1_layer)\n",
    "            self.dense_layer = tf.layers.dense(self.flatten_layer, 2, use_bias=False, name=\"dense_weights\")\n",
    "            \n",
    "            self.prob_layer = tf.nn.softmax(self.dense_layer)\n",
    "            self.log_prob_layer = tf.nn.log_softmax(self.dense_layer)\n",
    "                        \n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def model_variables(self):\n",
    "        return [x for x in tf.trainable_variables() if self.model_name in x.name]\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.session.run(self.prob_layer, feed_dict={self.input_layer: x})\n",
    "    \n",
    "    def fisher_vector_product(self, x, vector):\n",
    "        expected_log_prob = tf.reduce_sum(tf.stop_gradient(self.prob_layer) * self.log_prob_layer, 1)\n",
    "        log_prob_grad = get_flattened_gradients(expected_log_prob, self.model_variables())\n",
    "        grad_vector_product = tf.reduce_sum(log_prob_grad * vector)\n",
    "        fisher_vector_product = - get_flattened_gradients(grad_vector_product, self.model_variables())\n",
    "\n",
    "        return self.session.run(fisher_vector_product, feed_dict={self.input_layer: x})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "flappy_model = RL_Agent(\"test_model\")\n",
    "print flappy_model.predict(np.expand_dims(ob, axis=0))\n",
    "print flappy_model.model_variables()\n",
    "print flappy_model.fisher_vector_product(np.expand_dims(ob, axis=0), \n",
    "                                         tf.constant([1] * (9216 * 2 + 600), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.Variable([[1, 1], [1, 2]], dtype=tf.float32)\n",
    "y = tf.Variable([[2, 1]], dtype=tf.float32)\n",
    "z = tf.matmul(tf.matmul(y, x), tf.transpose(y))\n",
    "\n",
    "target_v = tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.float32)\n",
    "\n",
    "def get_padded_gradients(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return [g if g is not None else tf.zeros(v.shape)\n",
    "            for g, v in zip(grads, var_list)]\n",
    "\n",
    "def get_flattened_gradients(loss, var_list):\n",
    "    padded_gradients = get_padded_gradients(loss, var_list)\n",
    "    return tf.concat([tf.reshape(x, [-1]) for x in padded_gradients], 0)\n",
    "\n",
    "# grads = get_padded_gradients(z, [x, y])\n",
    "# flat_grad = tf.concat([tf.reshape(grads[i], [-1]) for i in range(len(grads))], 0)\n",
    "\n",
    "flat_grad = get_flattened_gradients(z, [x, y])\n",
    "flat_vars = tf.concat([tf.reshape(x, [-1]), tf.reshape(y, [-1])], 0)\n",
    "print flat_grad, flat_vars\n",
    "\n",
    "grad_vector_product = tf.reduce_sum(flat_grad * target_v)\n",
    "fisher_vector_product = get_flattened_gradients(grad_vector_product, [x, y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    cur_test = sess.run(fisher_vector_product)\n",
    "    \n",
    "    print cur_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
